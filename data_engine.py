"""
æ ¸å¿ƒæ•¸æ“šå¼•æ“æ¨¡çµ„ (Data Engine Module) v1.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ç¬¬ä¸€éšæ®µé‡æ§‹ï¼šè§£æ±ºæ•ˆèƒ½èˆ‡æ•¸æ“šé¡å‹å•é¡Œ

ğŸš€ ç‰¹æ€§ï¼š
  1. é«˜æ•ˆå¿«å–ï¼š@st.cache_data åŒ…è£è³‡æ–™è¼‰å…¥
  2. å‘é‡åŒ–é‹ç®—ï¼šç¦æ­¢ apply/loopï¼Œå…¨é¢ä½¿ç”¨ groupby å‘é‡åŒ–
  3. å¼·åˆ¶ AID å­—ä¸²ï¼šå…¨åŸŸç¢ºä¿ AID ç‚ºå­—ä¸²å‹åˆ¥
  4. Violin Plot æ¡æ¨£ï¼šå¤§æ•¸æ“šé›†è‡ªå‹•æŠ½æ¨£ 10%

ğŸ“Š è¨ˆç®—æŒ‡æ¨™ï¼š
  - ç¸½ç›ˆè™§ã€Scalp ç›ˆè™§ã€Scalp%ï¼ˆç­†æ•¸ä½”æ¯”ï¼‰
  - å‹ç‡ã€Sharpe Ratioã€MDD%
  - å–®ç­†ç›ˆè™§çš„ Q1, Median, Q3
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

import streamlit as st
import pandas as pd
import numpy as np
from typing import Tuple, Optional, List, Dict, Any
from io import BytesIO


# ==================== æ¬„ä½æ˜ å°„é…ç½® ====================
COLUMN_MAP = {
    'execution_time': 'Execution Time\näº¤æ˜“æ—¶é—´',
    'open_time': 'Open Time\nå¼€ä»“æ—¶é—´',
    'aid': 'AID\nç”¨æˆ·è´¦å·',
    'closed_pl': 'Closed P/L\nå¹³ä»“ç›ˆäº',
    'commission': 'Commission\næ‰‹ç»­è´¹',
    'swap': 'Swap\néš”å¤œåˆ©æ¯',
    'instrument': 'Instrument\näº¤æ˜“å“ç§',
    'business_type': 'Business Type\nä¸šåŠ¡ç±»å‹',
    'action': 'Action\näº¤æ˜“ç±»å‹',
    'volume': 'Volume\nå¼€ä»“æ•°é‡',
    'side': 'Side\näº¤æ˜“æ–¹å‘'
}


# ==================== è³‡æ–™è¼‰å…¥èˆ‡æ¸…æ´— ====================
@st.cache_data(show_spinner=False, ttl=3600)
def load_data(uploaded_files: List[Any]) -> Optional[pd.DataFrame]:
    """
    è¼‰å…¥ä¸¦é è™•ç†äº¤æ˜“æ•¸æ“š
    
    ğŸ”§ å¿«å–ç­–ç•¥ï¼š
    - ttl=3600ï¼šå¿«å– 1 å°æ™‚
    - show_spinner=Falseï¼šç”±å¤–å±¤æ§åˆ¶ spinner
    
    ğŸ“‹ è™•ç†æµç¨‹ï¼š
    1. è®€å– CSV/Excel æª”æ¡ˆ
    2. åˆä½µå»é‡
    3. æ™‚é–“æ¬„ä½è½‰æ›
    4. è¨ˆç®— Net_PL èˆ‡æŒå€‰æ™‚é–“
    5. â­ å¼·åˆ¶ AID è½‰å­—ä¸²ï¼ˆè§£æ±ºè¤‡è£½åŠŸèƒ½å¤±æ•ˆå•é¡Œï¼‰
    
    Args:
        uploaded_files: Streamlit ä¸Šå‚³çš„æª”æ¡ˆåˆ—è¡¨
        
    Returns:
        é è™•ç†å¾Œçš„ DataFrameï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    if not uploaded_files:
        return None
    
    dfs = []
    for uploaded_file in uploaded_files:
        try:
            # é‡ç½®æª”æ¡ˆæŒ‡æ¨™ï¼ˆé¿å…å¿«å–å•é¡Œï¼‰
            uploaded_file.seek(0)
            
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            dfs.append(df)
        except Exception as e:
            st.error(f"è®€å–æª”æ¡ˆ {uploaded_file.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
    
    if not dfs:
        return None
    
    # åˆä½µè³‡æ–™
    df = pd.concat(dfs, ignore_index=True)
    
    # ç§»é™¤ Total è¡Œ
    exec_col = COLUMN_MAP['execution_time']
    if exec_col in df.columns:
        df = df[df[exec_col] != 'Total'].copy()
    
    # å»é‡
    df = df.drop_duplicates()
    
    # æ¸…æ´—è³‡æ–™
    df = _clean_data(df)
    
    return df


def _clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    è³‡æ–™æ¸…æ´—æ ¸å¿ƒé‚è¼¯
    
    â­ é—œéµä¿®å¾©ï¼šå¼·åˆ¶ AID ç‚ºå­—ä¸²å‹åˆ¥
    """
    # ==================== 1. æ™‚é–“æ¬„ä½è½‰æ› ====================
    for col_key in ['execution_time', 'open_time']:
        col_name = COLUMN_MAP[col_key]
        if col_name in df.columns:
            df[col_name] = pd.to_datetime(df[col_name], errors='coerce')
    
    # ==================== 2. æ•¸å€¼æ¬„ä½å¡«è£œ ====================
    for col_key in ['closed_pl', 'commission', 'swap']:
        col_name = COLUMN_MAP[col_key]
        if col_name in df.columns:
            df[col_name] = pd.to_numeric(df[col_name], errors='coerce').fillna(0)
    
    # ==================== 3. è¨ˆç®— Net_PL ====================
    closed_pl = df.get(COLUMN_MAP['closed_pl'], 0)
    commission = df.get(COLUMN_MAP['commission'], 0)
    swap = df.get(COLUMN_MAP['swap'], 0)
    df['Net_PL'] = closed_pl + commission + swap
    
    # ==================== 4. è¨ˆç®—æŒå€‰æ™‚é–“ ====================
    exec_col = COLUMN_MAP['execution_time']
    open_col = COLUMN_MAP['open_time']
    
    if exec_col in df.columns and open_col in df.columns:
        exec_time = df[exec_col]
        open_time = df[open_col]
        
        # å‘é‡åŒ–è¨ˆç®—æŒå€‰ç§’æ•¸
        valid_mask = pd.notna(exec_time) & pd.notna(open_time)
        df['Hold_Seconds'] = np.where(
            valid_mask,
            (exec_time - open_time).dt.total_seconds(),
            np.nan
        )
        df['Hold_Minutes'] = df['Hold_Seconds'] / 60
    else:
        df['Hold_Seconds'] = np.nan
        df['Hold_Minutes'] = np.nan
    
    # ==================== 5. â­ å¼·åˆ¶ AID ç‚ºå­—ä¸² ====================
    aid_col = COLUMN_MAP['aid']
    if aid_col in df.columns:
        df[aid_col] = (
            df[aid_col]
            .astype(str)
            .str.replace(r'\.0$', '', regex=True)  # ç§»é™¤å°æ•¸é»
            .str.replace(',', '', regex=False)       # ç§»é™¤åƒåˆ†ä½
            .str.strip()                              # ç§»é™¤ç©ºç™½
        )
    
    return df


def filter_closing_trades(df: pd.DataFrame) -> pd.DataFrame:
    """
    éæ¿¾å¹³å€‰äº¤æ˜“
    
    Args:
        df: åŸå§‹ DataFrame
        
    Returns:
        åƒ…åŒ…å«å¹³å€‰äº¤æ˜“çš„ DataFrame
    """
    action_col = COLUMN_MAP['action']
    if action_col in df.columns:
        return df[df[action_col] == 'CLOSING'].copy()
    return df.copy()


# ==================== å‘é‡åŒ–æŒ‡æ¨™è¨ˆç®—å¼•æ“ ====================
@st.cache_data(show_spinner=False)
def get_client_metrics(
    df: pd.DataFrame,
    initial_balance: float = 10000.0,
    scalper_threshold_seconds: float = 300.0
) -> pd.DataFrame:
    """
    å‘é‡åŒ–è¨ˆç®—æ‰€æœ‰å®¢æˆ¶æŒ‡æ¨™
    
    ğŸš€ æ•ˆèƒ½å„ªåŒ–ï¼š
    - å®Œå…¨ç¦æ­¢ apply() å’Œ for loop
    - ä½¿ç”¨ groupby + å‘é‡åŒ–èšåˆ
    - å–®æ¬¡éæ­·è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™
    
    ğŸ“Š è¨ˆç®—æŒ‡æ¨™ï¼š
    - ç¸½ç›ˆè™§ (Net_PL)
    - Scalp ç›ˆè™§ (Scalp_PL)
    - Scalp% ç­†æ•¸ä½”æ¯” (Scalp_Pct)
    - å‹ç‡ (Win_Rate)
    - Sharpe Ratio
    - MDD%
    - Q1, Median, Q3 (å–®ç­†ç›ˆè™§åˆ†ä½æ•¸)
    
    Args:
        df: é è™•ç†å¾Œçš„ DataFrame
        initial_balance: åˆå§‹è³‡é‡‘
        scalper_threshold_seconds: Scalp é–€æª»ç§’æ•¸
        
    Returns:
        åŒ…å«æ‰€æœ‰å®¢æˆ¶æŒ‡æ¨™çš„ DataFrame
    """
    aid_col = COLUMN_MAP['aid']
    exec_col = COLUMN_MAP['execution_time']
    
    # éæ¿¾å¹³å€‰äº¤æ˜“
    closing_df = filter_closing_trades(df)
    
    if closing_df.empty or aid_col not in closing_df.columns:
        return pd.DataFrame()
    
    # ==================== å»ºç«‹ Scalp æ¨™è¨˜ ====================
    closing_df = closing_df.copy()
    closing_df['Is_Scalp'] = closing_df['Hold_Seconds'] < scalper_threshold_seconds
    closing_df['Is_Win'] = closing_df['Net_PL'] > 0
    closing_df['Scalp_PL'] = np.where(closing_df['Is_Scalp'], closing_df['Net_PL'], 0)
    
    # ==================== GroupBy å‘é‡åŒ–èšåˆ ====================
    grouped = closing_df.groupby(aid_col, sort=False)
    
    # åŸºç¤æŒ‡æ¨™
    metrics = grouped.agg(
        Net_PL=('Net_PL', 'sum'),
        Trade_Count=('Net_PL', 'count'),
        Win_Count=('Is_Win', 'sum'),
        Scalp_Count=('Is_Scalp', 'sum'),
        Scalp_PL=('Scalp_PL', 'sum'),
        PL_Mean=('Net_PL', 'mean'),
        PL_Std=('Net_PL', 'std'),
        Q1=('Net_PL', lambda x: x.quantile(0.25)),
        Median=('Net_PL', 'median'),
        Q3=('Net_PL', lambda x: x.quantile(0.75)),
    ).reset_index()
    
    # é‡å‘½å AID æ¬„ä½
    metrics = metrics.rename(columns={aid_col: 'AID'})
    
    # ==================== å‘é‡åŒ–è¨ˆç®—è¡ç”ŸæŒ‡æ¨™ ====================
    # Scalp%ï¼ˆç­†æ•¸ä½”æ¯”ï¼‰
    metrics['Scalp_Pct'] = np.where(
        metrics['Trade_Count'] > 0,
        (metrics['Scalp_Count'] / metrics['Trade_Count']) * 100,
        0
    )
    
    # å‹ç‡
    metrics['Win_Rate'] = np.where(
        metrics['Trade_Count'] > 0,
        (metrics['Win_Count'] / metrics['Trade_Count']) * 100,
        0
    )
    
    # Sharpe Ratioï¼ˆéœ€è¦è‡³å°‘ 3 ç­†äº¤æ˜“ä¸”æ¨™æº–å·® > 0ï¼‰
    metrics['Sharpe'] = np.where(
        (metrics['Trade_Count'] >= 3) & (metrics['PL_Std'] > 0),
        metrics['PL_Mean'] / metrics['PL_Std'],
        0
    )
    
    # IQR
    metrics['IQR'] = metrics['Q3'] - metrics['Q1']
    
    # ==================== MDD% è¨ˆç®—ï¼ˆéœ€è¦ç‰¹æ®Šè™•ç†ï¼‰====================
    metrics['MDD_Pct'] = _calculate_mdd_vectorized(
        closing_df, aid_col, exec_col, initial_balance
    )
    
    # ==================== æ•´ç†è¼¸å‡ºæ¬„ä½ ====================
    output_cols = [
        'AID', 'Net_PL', 'Trade_Count', 'Scalp_PL', 'Scalp_Pct',
        'Win_Rate', 'Sharpe', 'MDD_Pct', 'Q1', 'Median', 'Q3', 'IQR'
    ]
    
    # ç¢ºä¿æ‰€æœ‰æ¬„ä½å­˜åœ¨
    for col in output_cols:
        if col not in metrics.columns:
            metrics[col] = 0
    
    # å››æ¨äº”å…¥
    numeric_cols = ['Net_PL', 'Scalp_PL', 'Scalp_Pct', 'Win_Rate', 
                    'Sharpe', 'MDD_Pct', 'Q1', 'Median', 'Q3', 'IQR']
    for col in numeric_cols:
        metrics[col] = metrics[col].round(2)
    
    # â­ ç¢ºä¿ AID ç‚ºå­—ä¸²
    metrics['AID'] = metrics['AID'].astype(str)
    
    return metrics[output_cols]


def _calculate_mdd_vectorized(
    df: pd.DataFrame,
    aid_col: str,
    exec_col: str,
    initial_balance: float
) -> pd.Series:
    """
    å‘é‡åŒ–è¨ˆç®— MDD%
    
    ğŸ’¡ ç­–ç•¥ï¼š
    - ä½¿ç”¨ groupby + apply ä½†å…§éƒ¨ç‚ºå‘é‡åŒ–é‹ç®—
    - å°æ–¼ç„¡æ³•å®Œå…¨å‘é‡åŒ–çš„ MDDï¼Œé€™æ˜¯æœ€ä½³æŠ˜è¡·
    
    Returns:
        ä»¥ AID ç‚ºç´¢å¼•çš„ MDD% Series
    """
    def calc_mdd_for_group(group: pd.DataFrame) -> float:
        if len(group) < 2:
            return 0.0
        
        # æŒ‰æ™‚é–“æ’åº
        sorted_group = group.sort_values(exec_col)
        
        # ç´¯è¨ˆç›ˆè™§
        cumulative_pl = sorted_group['Net_PL'].cumsum()
        
        # æ¬Šç›Šæ›²ç·š
        equity = initial_balance + cumulative_pl
        
        # æ­·å²æœ€é«˜é»
        running_max = equity.cummax()
        
        # å›æ’¤ç™¾åˆ†æ¯”
        drawdown_pct = np.where(
            running_max != 0,
            (equity - running_max) / running_max * 100,
            0
        )
        
        # æœ€å¤§å›æ’¤ï¼ˆå–çµ•å°å€¼ï¼‰
        return abs(np.min(drawdown_pct))
    
    mdd_series = df.groupby(aid_col, sort=False).apply(
        calc_mdd_for_group, include_groups=False
    )
    
    return mdd_series.values


# ==================== Violin Plot æ¡æ¨£é‚è¼¯ ====================
def get_violin_sample(
    df: pd.DataFrame,
    max_points: int = 5000,
    sample_ratio: float = 0.1,
    random_state: int = 42
) -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    ç‚º Violin Plot æº–å‚™æ¡æ¨£æ•¸æ“š
    
    ğŸ“Š ç­–ç•¥ï¼š
    - è‹¥æ•¸æ“šé» > max_pointsï¼Œå‰‡æŠ½æ¨£ sample_ratio (10%)
    - åŒæ™‚è¿”å›å®Œæ•´çµ±è¨ˆæ•¸æ“šï¼ˆåŸºæ–¼å…¨é‡è³‡æ–™è¨ˆç®—ï¼‰
    
    Args:
        df: é è™•ç†å¾Œçš„ DataFrame
        max_points: è§¸ç™¼æ¡æ¨£çš„é–€æª»
        sample_ratio: æ¡æ¨£æ¯”ä¾‹
        random_state: éš¨æ©Ÿç¨®å­ï¼ˆç¢ºä¿å¯é‡ç¾ï¼‰
        
    Returns:
        (æ¡æ¨£å¾Œçš„ DataFrame, çµ±è¨ˆå­—å…¸)
    """
    aid_col = COLUMN_MAP['aid']
    closing_df = filter_closing_trades(df)
    
    # è¨ˆç®—æ¯å€‹å®¢æˆ¶çš„ç¸½ç›ˆè™§
    aid_pl = closing_df.groupby(aid_col, sort=False)['Net_PL'].sum().reset_index()
    aid_pl.columns = ['AID', 'Net_PL']
    
    # â­ ç¢ºä¿ AID ç‚ºå­—ä¸²
    aid_pl['AID'] = aid_pl['AID'].astype(str)
    
    # ==================== è¨ˆç®—å®Œæ•´çµ±è¨ˆï¼ˆåŸºæ–¼å…¨é‡è³‡æ–™ï¼‰====================
    stats = _calculate_violin_stats(aid_pl)
    
    # ==================== æ¡æ¨£é‚è¼¯ ====================
    n_points = len(aid_pl)
    
    if n_points > max_points:
        sample_size = int(n_points * sample_ratio)
        sample_df = aid_pl.sample(n=sample_size, random_state=random_state)
        stats['is_sampled'] = True
        stats['sample_size'] = sample_size
        stats['original_size'] = n_points
    else:
        sample_df = aid_pl
        stats['is_sampled'] = False
        stats['sample_size'] = n_points
        stats['original_size'] = n_points
    
    return sample_df, stats


def _calculate_violin_stats(aid_pl: pd.DataFrame) -> Dict[str, Any]:
    """
    è¨ˆç®— Violin Plot çµ±è¨ˆæ•¸æ“š
    
    Returns:
        çµ±è¨ˆå­—å…¸
    """
    net_pl = aid_pl['Net_PL']
    
    count = len(aid_pl)
    mean_val = net_pl.mean()
    median_val = net_pl.median()
    std_val = net_pl.std()
    q1 = net_pl.quantile(0.25)
    q3 = net_pl.quantile(0.75)
    iqr = q3 - q1
    min_val = net_pl.min()
    max_val = net_pl.max()
    
    # é›¢ç¾¤å€¼ç¯„åœ
    lower_fence = q1 - 1.5 * iqr
    upper_fence = q3 + 1.5 * iqr
    
    # ç›ˆè™§çµ±è¨ˆ
    profitable = (net_pl > 0).sum()
    losing = (net_pl <= 0).sum()
    outliers = ((net_pl < lower_fence) | (net_pl > upper_fence)).sum()
    
    return {
        'count': count,
        'mean': mean_val,
        'median': median_val,
        'std': std_val,
        'q1': q1,
        'q3': q3,
        'iqr': iqr,
        'min': min_val,
        'max': max_val,
        'lower_fence': lower_fence,
        'upper_fence': upper_fence,
        'profitable': profitable,
        'losing': losing,
        'outliers': outliers
    }


# ==================== è¼”åŠ©å‡½æ•¸ ====================
def get_aid_column_name() -> str:
    """è¿”å› AID æ¬„ä½åç¨±"""
    return COLUMN_MAP['aid']


def get_column_map() -> Dict[str, str]:
    """è¿”å›å®Œæ•´æ¬„ä½æ˜ å°„"""
    return COLUMN_MAP.copy()


# ==================== æ“´å±•æŒ‡æ¨™è¨ˆç®—ï¼ˆè‹±é›„æ¦œç”¨ï¼‰====================
@st.cache_data(show_spinner=False)
def get_hero_metrics(
    df: pd.DataFrame,
    initial_balance: float = 10000.0,
    scalper_threshold_seconds: float = 300.0,
    filter_positive: bool = True,
    min_scalp_pct: Optional[float] = None,
    min_scalp_pl: Optional[float] = None,
    min_pnl: Optional[float] = None,
    min_winrate: Optional[float] = None,
    min_sharpe: Optional[float] = None,
    max_mdd: Optional[float] = None,
    top_n: int = 20
) -> pd.DataFrame:
    """
    è¨ˆç®—è‹±é›„æ¦œæŒ‡æ¨™ï¼ˆå«éæ¿¾é‚è¼¯ï¼‰
    
    ğŸ“Š å®Œæ•´æŒ‡æ¨™ï¼š
    - AID, ç›ˆè™§, Scalpç›ˆè™§, Scalp%
    - Sharpe, MDD%, Q1, Median, Q3, IQR
    - P.Exp (Profit Expectancy), PF (Profit Factor), Rec.F (Recovery Factor)
    - å‹ç‡%, ç­†æ•¸
    
    Args:
        df: é è™•ç†å¾Œçš„ DataFrame
        initial_balance: åˆå§‹è³‡é‡‘
        scalper_threshold_seconds: Scalp é–€æª»ç§’æ•¸
        filter_positive: æ˜¯å¦åƒ…é¡¯ç¤ºæ­£ç›ˆè™§
        min_scalp_pct: æœ€ä½ Scalp%
        min_scalp_pl: æœ€ä½ Scalp ç›ˆè™§
        min_pnl: æœ€ä½ç›ˆè™§
        min_winrate: æœ€ä½å‹ç‡
        min_sharpe: æœ€ä½ Sharpe
        max_mdd: æœ€é«˜ MDD%
        top_n: è¿”å›å‰ N å
        
    Returns:
        è‹±é›„æ¦œ DataFrame
    """
    aid_col = COLUMN_MAP['aid']
    exec_col = COLUMN_MAP['execution_time']
    closed_pl_col = COLUMN_MAP['closed_pl']
    
    closing_df = filter_closing_trades(df)
    
    if closing_df.empty:
        return pd.DataFrame()
    
    # ==================== å»ºç«‹æ¨™è¨˜æ¬„ä½ ====================
    closing_df = closing_df.copy()
    closing_df['Is_Scalp'] = closing_df['Hold_Seconds'] < scalper_threshold_seconds
    closing_df['Is_Win'] = closing_df['Net_PL'] > 0
    closing_df['Is_Loss'] = closing_df['Net_PL'] < 0
    closing_df['Scalp_PL'] = np.where(closing_df['Is_Scalp'], closing_df['Net_PL'], 0)
    closing_df['Win_PL'] = np.where(closing_df['Is_Win'], closing_df['Net_PL'], 0)
    closing_df['Loss_PL'] = np.where(closing_df['Is_Loss'], closing_df['Net_PL'].abs(), 0)
    
    # ==================== GroupBy èšåˆ ====================
    grouped = closing_df.groupby(aid_col, sort=False)
    
    metrics = grouped.agg(
        Net_PL=('Net_PL', 'sum'),
        Trade_Count=('Net_PL', 'count'),
        Win_Count=('Is_Win', 'sum'),
        Loss_Count=('Is_Loss', 'sum'),
        Scalp_Count=('Is_Scalp', 'sum'),
        Scalp_PL=('Scalp_PL', 'sum'),
        Total_Wins=('Win_PL', 'sum'),
        Total_Losses=('Loss_PL', 'sum'),
        PL_Mean=('Net_PL', 'mean'),
        PL_Std=('Net_PL', 'std'),
        Q1=('Net_PL', lambda x: x.quantile(0.25)),
        Median=('Net_PL', 'median'),
        Q3=('Net_PL', lambda x: x.quantile(0.75)),
    ).reset_index()
    
    metrics = metrics.rename(columns={aid_col: 'AID'})
    
    # ==================== è¨ˆç®—è¡ç”ŸæŒ‡æ¨™ ====================
    tc = metrics['Trade_Count']
    
    # Scalp%
    metrics['Scalp_Pct'] = np.where(tc > 0, (metrics['Scalp_Count'] / tc) * 100, 0)
    
    # å‹ç‡
    metrics['Win_Rate'] = np.where(tc > 0, (metrics['Win_Count'] / tc) * 100, 0)
    
    # Sharpe
    metrics['Sharpe'] = np.where(
        (tc >= 3) & (metrics['PL_Std'] > 0),
        metrics['PL_Mean'] / metrics['PL_Std'],
        0
    )
    
    # IQR
    metrics['IQR'] = metrics['Q3'] - metrics['Q1']
    
    # Profit Factor
    metrics['PF'] = np.where(
        metrics['Total_Losses'] > 0,
        metrics['Total_Wins'] / metrics['Total_Losses'],
        np.where(metrics['Total_Wins'] > 0, 5.0, 0.0)
    )
    
    # Profit Expectancy
    win_prob = metrics['Win_Count'] / tc
    loss_prob = metrics['Loss_Count'] / tc
    avg_win = np.where(metrics['Win_Count'] > 0, metrics['Total_Wins'] / metrics['Win_Count'], 0)
    avg_loss = np.where(metrics['Loss_Count'] > 0, metrics['Total_Losses'] / metrics['Loss_Count'], 0)
    metrics['P_Exp'] = (win_prob * avg_win) - (loss_prob * avg_loss)
    
    # MDD% è¨ˆç®—
    mdd_values = _calculate_mdd_for_hero(closing_df, aid_col, exec_col, initial_balance)
    metrics = metrics.merge(mdd_values, on='AID', how='left')
    metrics['MDD_Pct'] = metrics['MDD_Pct'].fillna(0)
    
    # Recovery Factor
    metrics['Rec_F'] = np.where(
        metrics['Max_DD_Abs'] > 0,
        metrics['Net_PL'] / metrics['Max_DD_Abs'],
        np.where(metrics['Net_PL'] > 0, metrics['Net_PL'], 0)
    )
    
    # ==================== æ‡‰ç”¨éæ¿¾æ¢ä»¶ ====================
    mask = pd.Series(True, index=metrics.index)
    
    if filter_positive:
        mask &= metrics['Net_PL'] > 0
    if min_scalp_pct is not None:
        mask &= metrics['Scalp_Pct'] >= float(min_scalp_pct)
    if min_scalp_pl is not None:
        mask &= metrics['Scalp_PL'] >= float(min_scalp_pl)
    if min_pnl is not None:
        mask &= metrics['Net_PL'] >= float(min_pnl)
    if min_winrate is not None:
        mask &= metrics['Win_Rate'] >= float(min_winrate)
    if min_sharpe is not None:
        mask &= metrics['Sharpe'] >= float(min_sharpe)
    if max_mdd is not None:
        mask &= metrics['MDD_Pct'] <= float(max_mdd)
    
    filtered = metrics[mask].copy()
    
    # ==================== æ’åºä¸¦å– Top N ====================
    filtered = filtered.sort_values('Net_PL', ascending=False).head(top_n)
    
    # ==================== æ•´ç†è¼¸å‡º ====================
    output_cols = [
        'AID', 'Net_PL', 'Scalp_PL', 'Scalp_Pct', 'Sharpe', 'MDD_Pct',
        'Q1', 'Median', 'Q3', 'IQR', 'P_Exp', 'PF', 'Rec_F', 'Win_Rate', 'Trade_Count'
    ]
    
    # é‡å‘½åç‚ºä¸­æ–‡
    rename_map = {
        'Net_PL': 'ç›ˆè™§',
        'Scalp_PL': 'Scalpç›ˆè™§',
        'Scalp_Pct': 'Scalp%',
        'MDD_Pct': 'MDD%',
        'P_Exp': 'P. Exp',
        'Rec_F': 'Rec.F',
        'Win_Rate': 'å‹ç‡%',
        'Trade_Count': 'ç­†æ•¸'
    }
    
    result = filtered[output_cols].copy()
    result = result.rename(columns=rename_map)
    
    # å››æ¨äº”å…¥
    numeric_cols = ['ç›ˆè™§', 'Scalpç›ˆè™§', 'Scalp%', 'Sharpe', 'MDD%',
                    'Q1', 'Median', 'Q3', 'IQR', 'P. Exp', 'PF', 'Rec.F', 'å‹ç‡%']
    for col in numeric_cols:
        if col in result.columns:
            result[col] = result[col].round(2)
    
    # â­ ç¢ºä¿ AID ç‚ºå­—ä¸²
    result['AID'] = result['AID'].astype(str)
    
    return result


def _calculate_mdd_for_hero(
    df: pd.DataFrame,
    aid_col: str,
    exec_col: str,
    initial_balance: float
) -> pd.DataFrame:
    """
    ç‚ºè‹±é›„æ¦œè¨ˆç®— MDD% å’Œ Max_DD_Abs
    
    Returns:
        åŒ…å« AID, MDD_Pct, Max_DD_Abs çš„ DataFrame
    """
    results = []
    
    for aid, group in df.groupby(aid_col, sort=False):
        if len(group) < 2:
            results.append({'AID': str(aid), 'MDD_Pct': 0.0, 'Max_DD_Abs': 0.0})
            continue
        
        sorted_group = group.sort_values(exec_col)
        cumulative_pl = sorted_group['Net_PL'].cumsum()
        equity = initial_balance + cumulative_pl
        running_max = equity.cummax()
        
        # MDD%
        drawdown_pct = np.where(
            running_max != 0,
            (equity - running_max) / running_max * 100,
            0
        )
        mdd_pct = abs(np.min(drawdown_pct))
        
        # Max DD Abs
        max_dd_abs = abs((equity - running_max).min())
        
        results.append({
            'AID': str(aid),
            'MDD_Pct': round(mdd_pct, 2),
            'Max_DD_Abs': round(max_dd_abs, 2)
        })
    
    return pd.DataFrame(results)


# ==================== æ¸¬è©¦å€å¡Š ====================
if __name__ == "__main__":
    print("=" * 60)
    print("æ ¸å¿ƒæ•¸æ“šå¼•æ“æ¨¡çµ„ v1.0")
    print("=" * 60)
    print("\nâœ… æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
    print("\nğŸ“‹ å¯ç”¨å‡½æ•¸ï¼š")
    print("  - load_data(uploaded_files)")
    print("  - get_client_metrics(df, initial_balance, scalper_threshold_seconds)")
    print("  - get_violin_sample(df, max_points, sample_ratio)")
    print("  - get_hero_metrics(df, ...)")
    print("  - filter_closing_trades(df)")
    print("  - get_column_map()")
    print("\nğŸ”§ æ¬„ä½æ˜ å°„ï¼š")
    for key, value in COLUMN_MAP.items():
        print(f"  {key}: {value}")
